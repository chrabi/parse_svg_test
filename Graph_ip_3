# Function to process data and save to CSV
def process_and_save_to_csv(data: Dict[str, Any], logger: logging.Logger) -> bool:
    """
    Processes data from GraphQL response and saves to CSV file.
    
    Args:
        data: Dictionary with response data
        logger: Logger object
        
    Returns:
        True if operation succeeded, False otherwise
    """
    try:
        # Check if data contains expected structure
        if not data or "data" not in data:
            logger.error("No data to process")
            return False
            
        # Adjust this path to your GraphQL data structure
        server_data = data.get("data", {}).get("applicationXGenericHardwar", [])
        
        if not server_data:
            logger.warning("No records to save")
            return False
            
        # Prepare output directory
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = f"output/{timestamp}/Inventory"
        os.makedirs(output_dir, exist_ok=True)
        
        # CSV filename
        current_date = datetime.now().strftime("%Y%m%d")
        output_file = f"{output_dir}/Inventory_{current_date}.csv"
        
        # CSV headers
        fieldnames = ["Id", "ServerName", "Serial", "AppId", "AppName", "TimestampEpoch", "IP", "ServerCount"]
        
        # Prepare data to save
        csv_data = []
        
        logger.info(f"Processing {len(server_data)} records")
        
        for server in server_data:
            try:
                # Safely retrieve data using safe_get function
                server_row = {
                    "Id": safe_get(server, "HardwareInfo", "Idresource"),
                    "ServerName": safe_get(server, "HardwareInfo", "serverName"),
                    "Serial": safe_get(server, "HardwareInfo", "serial"),
                    "AppId": safe_get(server, "applicationId"),
                    "AppName": f"App-{safe_get(server, 'applicationId')}",
                    "TimestampEpoch": int(time.time()),# Funkcja do pobierania danych z HP OneView na podstawie adresu IP# Function to safely retrieve values from nested dictionaries
def safe_get(data, *keys, default=""):
    """
    Safely retrieves values from nested dictionaries.
    Returns a default value if any element in the path is None.
    
    Args:
        data: Dictionary data
        *keys: Keys for nested access
        default: Default value returned in case of error
        
    Returns:
        Value from the nested dictionary or default value
    """
    current = data
    for key in keys:
        if current is None or not isinstance(current, dict):
            return default
        current = current.get(key)
    return current if current is not None else default        
    except Exception as e:
        logger.error(f"Błąd podczas przetwarzania danych: {str(e)}")
        return False#!/usr/bin/env python3.12
"""
Script for executing GraphQL queries and saving results to CSV.
Query concerns servers from applicationXGenericHardwar with filter on applicationId (175442).
"""

import csv
import json
import logging
import os
import sys
import time
import socket
import subprocess
import concurrent.futures
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple

import requests
from requests.exceptions import RequestException

# Logger configuration
def setup_logger() -> logging.Logger:
    """Configure the logging system."""
    logger = logging.getLogger("graphql_export")
    logger.setLevel(logging.INFO)
    
    # Handler for console logs
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(console_format)
    
    # Handler for file logs
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    file_handler = logging.FileHandler(f"{log_dir}/graphql_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    file_handler.setLevel(logging.DEBUG)
    file_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(file_format)
    
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    return logger

# Function for pinging a server and returning its IP address (thread version)
def ping_server(server_name: str) -> Tuple[str, str]:
    """
    Pings the given server and returns its name and IP address.
    Performs 2 ping attempts for increased reliability.
    
    Args:
        server_name: Server name to ping
        
    Returns:
        Tuple (server_name, ip_address), where ip_address can be an empty string in case of failure
    """
    if not server_name:
        return (server_name, "")
        
    try:
        # Try to resolve server name to IP address
        try:
            ip_address = socket.gethostbyname(server_name)
            return (server_name, ip_address)
        except socket.gaierror:
            # If name resolution fails, try to ping
            for attempt in range(1, 3):  # 2 ping attempts
                # Determine ping command based on operating system
                if sys.platform.startswith('win'):
                    ping_cmd = ["ping", "-n", "1", "-w", "2000", server_name]
                else:
                    ping_cmd = ["ping", "-c", "1", "-W", "2", server_name]
                    
                # Execute ping command
                process = subprocess.Popen(ping_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                output, error = process.communicate()
                output_str = output.decode('utf-8', errors='ignore')
                
                # Parse ping result to extract IP address
                if process.returncode == 0:
                    # Try to extract IP address from ping result
                    import re
                    ip_pattern = r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'
                    ip_matches = re.findall(ip_pattern, output_str)
                    if ip_matches:
                        ip_address = ip_matches[0]
                        return (server_name, ip_address)
                elif attempt < 2:  # If this is not the last attempt, wait before the next one
                    time.sleep(1)  # Wait 1 second before the next attempt
            
            return (server_name, "")
    except Exception:
        return (server_name, "")

# Function for multithreaded server pinging
def get_server_ips_parallel(server_names: List[str], max_workers: int, logger: logging.Logger) -> Dict[str, str]:
    """
    Pings multiple servers in parallel and returns a mapping of server names to IP addresses.
    
    Args:
        server_names: List of server names to ping
        max_workers: Maximum number of parallel threads
        logger: Logger object
        
    Returns:
        Dictionary {server_name: ip_address}
    """
    if not server_names:
        return {}
        
    unique_names = list(set(server_names))  # Remove duplicates
    results = {}
    
    logger.info(f"Starting parallel pinging of {len(unique_names)} unique servers using {max_workers} threads")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Start all pinging tasks
        future_to_server = {executor.submit(ping_server, name): name for name in unique_names}
        
        # Process results as they complete
        completed = 0
        for future in concurrent.futures.as_completed(future_to_server):
            server_name, ip = future.result()
            results[server_name] = ip
            
            completed += 1
            if completed % 10 == 0 or completed == len(unique_names):
                logger.info(f"Pinging progress: {completed}/{len(unique_names)} ({round(completed/len(unique_names)*100)}%)")
    
    # Results summary
    success_count = sum(1 for ip in results.values() if ip)
    logger.info(f"Pinging completed. Found IP addresses for {success_count}/{len(unique_names)} servers")
    
    return results

# Maintained old implementation for compatibility and single calls
def get_server_ip(server_name: str, logger: logging.Logger) -> str:
    """
    Pings the given server and returns its IP address.
    Performs 2 ping attempts for increased reliability.
    
    Args:
        server_name: Server name to ping
        logger: Logger object
        
    Returns:
        Server IP address or an empty string in case of error
    """
    if not server_name:
        return ""
        
    try:
        logger.info(f"Pinging server: {server_name}")
        server_name, ip = ping_server(server_name)
        if ip:
            logger.info(f"Found IP address for {server_name}: {ip}")
        else:
            logger.warning(f"Failed to get IP address for server {server_name}")
        return ip
    except Exception as e:
        logger.error(f"Error while pinging server {server_name}: {str(e)}")
        return ""
def get_oneview_server_count_by_ip(ip_address: str, logger: logging.Logger) -> int:
    """
    Pobiera liczbę serwerów z HP OneView na podstawie adresu IP.
    
    Args:
        ip_address: Adres IP serwera OneView
        logger: Obiekt loggera
        
    Returns:
        Liczba serwerów lub 125 (domyślna wartość) w przypadku błędu
    """
    if not ip_address:
        logger.error("Brak adresu IP dla OneView, zwracam domyślną wartość 125")
        return 125
        
    try:
        # URL do uwierzytelniania HP OneView z użyciem podanego IP
        oneview_url = f"https://{ip_address}"
        auth_url = f"{oneview_url}/rest/login-sessions"
        server_hardware_url = f"{oneview_url}/rest/server-hardware"
        
        logger.info(f"Łączenie z HP OneView pod adresem: {oneview_url}")
        
        # Dane uwierzytelniające - zastąp właściwymi danymi
        auth_data = {
            "userName": "admin",
            "password": "password"
        }
        
        # Nagłówki dla żądania
        headers = {
            "Content-Type": "application/json",
            "X-API-Version": "800"
        }
        
        # Wykonanie żądania uwierzytelniającego
        auth_response = requests.post(auth_url, headers=headers, json=auth_data, verify=False, timeout=30)
        
        if auth_response.status_code != 200:
            logger.error(f"Błąd uwierzytelniania HP OneView: {auth_response.status_code}")
            return 125
            
        # Pobranie SessionID
        session_id = auth_response.json().get("sessionID")
        
        if not session_id:
            logger.error("Nie udało się pobrać SessionID z HP OneView")
            return 125
            
        logger.info("Uwierzytelnienie w HP OneView powiodło się")
        
        # Aktualizacja nagłówków o token sesji
        headers["Auth"] = session_id
        
        # Wykonanie żądania do /rest/server-hardware
        servers_response = requests.get(server_hardware_url, headers=headers, verify=False, timeout=30)
        
        if servers_response.status_code != 200:
            logger.error(f"Błąd pobierania danych o serwerach: {servers_response.status_code}")
            return 125
            
        # Pobranie całkowitej liczby serwerów
        total_servers = servers_response.json().get("total", 125)
        
        logger.info(f"Pobrano informację o liczbie serwerów z {ip_address}: {total_servers}")
        
        return total_servers
    except Exception as e:
        logger.error(f"Błąd podczas pobierania danych z HP OneView pod {ip_address}: {str(e)}")
        return 125

# Function to execute GraphQL query
def execute_graphql_query(url: str, query: str, variables: Dict[str, Any], logger: logging.Logger) -> Optional[Dict[str, Any]]:
    """
    Executes a GraphQL query and returns the result.
    
    Args:
        url: GraphQL endpoint URL
        query: GraphQL query text
        variables: Variables for the query
        logger: Logger object
        
    Returns:
        Dictionary with response or None in case of error
    """
    headers = {
        "Content-Type": "application/json",
        # Add additional headers here if required (e.g., authorization)
        # "Authorization": "Bearer YOUR_TOKEN"
    }
    
    payload = {
        "query": query,
        "variables": variables
    }
    
    try:
        logger.info(f"Executing GraphQL query with variables: {variables}")
        response = requests.post(url, json=payload, headers=headers)
        response.raise_for_status()
        
        data = response.json()
        
        if "errors" in data:
            logger.error(f"GraphQL error: {data['errors']}")
            return None
            
        return data
        
    except RequestException as e:
        logger.error(f"Error while executing query: {str(e)}")
        return None
    except ValueError as e:
        logger.error(f"Error while parsing JSON response: {str(e)}")
        return None

# Funkcja do przetwarzania danych i zapisywania do CSV
def process_and_save_to_csv(data: Dict[str, Any], logger: logging.Logger) -> bool:
    """
    Przetwarza dane z odpowiedzi GraphQL i zapisuje do pliku CSV.
    
    Args:
        data: Słownik z danymi odpowiedzi
        logger: Obiekt loggera
        
    Returns:
        True jeśli operacja się powiodła, False w przeciwnym wypadku
    """
    try:
        # Sprawdzenie, czy dane zawierają oczekiwaną strukturę
        if not data or "data" not in data:
            logger.error("Brak danych do przetworzenia")
            return False
            
        # Dostosuj tę ścieżkę do struktury Twoich danych GraphQL
        server_data = data.get("data", {}).get("applicationXGenericHardwar", [])
        
        if not server_data:
            logger.warning("Brak rekordów do zapisania")
            return False
            
        # Przygotowanie katalogu wyjściowego
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = f"output/{timestamp}/Inventory"
        os.makedirs(output_dir, exist_ok=True)
        
        # Nazwa pliku CSV
        current_date = datetime.now().strftime("%Y%m%d")
        output_file = f"{output_dir}/Inventory_{current_date}.csv"
        
        # Nagłówki CSV
        fieldnames = ["Id", "ServerName", "Serial", "AppId", "AppName", "TimestampEpoch", "IP", "ServerCount"]
        
        # Przygotowanie danych do zapisania
        csv_data = []
        
        logger.info(f"Przetwarzanie {len(server_data)} rekordów")
        
        for server in server_data:
            try:
                # Bezpieczne pobieranie danych z wykorzystaniem funkcji safe_get
                server_row = {
                    "Id": safe_get(server, "HardwareInfo", "Idresource"),
                    "ServerName": safe_get(server, "HardwareInfo", "serverName"),
                    "Serial": safe_get(server, "HardwareInfo", "serial"),
                    "AppId": safe_get(server, "applicationId"),
                    "AppName": f"App-{safe_get(server, 'applicationId')}",
                    "TimestampEpoch": int(time.time())
                }
                    csv_data.append(server_row)
            except Exception as e:
                logger.warning(f"Błąd podczas przetwarzania rekordu: {str(e)}. Kontynuowanie z kolejnymi rekordami.")
                continue
            csv_data.append(server_row)
        
        # Pobierz nazwy wszystkich serwerów do pingowania
        server_names = [server_row.get("ServerName") for server_row in csv_data if server_row.get("ServerName")]
        
        # Wykonaj równoległe pingowanie wszystkich serwerów
        max_workers = min(32, len(server_names))  # Maksymalnie 32 wątki, lub mniej jeśli mamy mniej serwerów
        ip_results = get_server_ips_parallel(server_names, max_workers, logger)
        
        # Aktualizuj rekordy CSV z adresami IP
        first_server_with_ip = None
        for server_row in csv_data:
            server_name = server_row.get("ServerName")
            if server_name and server_name in ip_results:
                server_ip = ip_results[server_name]
                server_row["IP"] = server_ip
                if server_ip and not first_server_with_ip:
                    first_server_with_ip = server_ip
        
        # Pobierz liczbę serwerów z HP OneView na podstawie pierwszego znalezionego adresu IP
        server_count = 125  # Domyślna wartość
        if first_server_with_ip:
            server_count = get_oneview_server_count_by_ip(first_server_with_ip, logger)
            logger.info(f"Liczba serwerów z HP OneView pod adresem {first_server_with_ip}: {server_count}")
        else:
            logger.warning("Nie znaleziono żadnego adresu IP, używam domyślnej wartości 125 dla liczby serwerów")
            
        # Dodaj liczbę serwerów z OneView do każdego rekordu
        for server_row in csv_data:
            server_row["ServerCount"] = server_count
        # Zapisanie do CSV
        with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:
            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(csv_data)
            
        logger.info(f"Zapisano {len(csv_data)} rekordów do pliku {output_file}")
        return True

def main():
    """Główna funkcja programu."""
    logger = setup_logger()
    logger.info("Rozpoczęcie wykonywania skryptu")
    start_time = time.time()  # Początek pomiaru czasu
    
    # URL endpointu GraphQL - zamień na właściwy adres
    graphql_url = "https://example.com/graphql"
    
    # Zapytanie GraphQL
    query = """
    query GetGenericHardware($page: Int!, $size: Int!, $applicationId: String!) {
      applicationXGenericHardwar(page: $page, size: $size, filter: { 
        applicationId: { equals: $applicationId }
      }) {
        applicationId
        driftId
        HardwareInfo {
          Idresource
          serverName
          serial
        }
      }
    }
    """
    
    # Zmienne dla zapytania - podstawowe wartości
    base_variables = {
        "size": 100,
        "applicationId": "175442"
    }
    
    try:
        # Lista na wszystkie pobrane dane
        all_results = []
        
        # Wykonanie zapytania dla stron od 1 do 4
        for page_num in range(1, 5):
            logger.info(f"Pobieranie danych dla strony {page_num}")
            
            # Aktualizacja numeru strony w zmiennych
            variables = {**base_variables, "page": page_num}
            
            # Wykonanie zapytania dla bieżącej strony
            result = execute_graphql_query(graphql_url, query, variables, logger)
            
            if not result:
                logger.error(f"Nie udało się wykonać zapytania GraphQL dla strony {page_num}")
                continue
            
            # Wyodrębnienie danych z odpowiedzi
            server_data = safe_get(result, "data", "applicationXGenericHardwar", default=[])
            
            if not server_data:
                logger.warning(f"Brak rekordów na stronie {page_num}")
                continue
                
            logger.info(f"Pobrano {len(server_data)} rekordów ze strony {page_num}")
            all_results.extend(server_data)
        
        # Sprawdzenie, czy udało się pobrać jakiekolwiek dane
        if not all_results:
            logger.error("Nie udało się pobrać żadnych danych ze wszystkich stron")
            sys.exit(1)
            
        logger.info(f"Łącznie pobrano {len(all_results)} rekordów ze wszystkich stron")
        
        # Przygotowanie danych w formacie odpowiednim dla process_and_save_to_csv
        combined_data = {"data": {"applicationXGenericHardwar": all_results}}
        
        # Przetworzenie i zapis do CSV
        if not process_and_save_to_csv(combined_data, logger):
            logger.error("Nie udało się zapisać danych do pliku CSV")
            sys.exit(1)
            
        logger.info("Skrypt zakończył działanie pomyślnie")
        total_time = time.time() - start_time
        logger.info(f"Całkowity czas wykonania: {total_time:.2f} sekund")
        
    except Exception as e:
        logger.critical(f"Wystąpił nieoczekiwany błąd: {str(e)}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()
