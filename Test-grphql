# Function to check Dell OME connections in parallel
def check_dell_ome_parallel(ip_addresses: List[str], max_workers: int, logger: logging.Logger) -> Dict[str, Tuple[bool, int]]:
    """
    Checks Dell OME connections for multiple IP addresses in parallel.
    
    Args:
        ip_addresses: List of IP addresses to check
        max_workers: Maximum number of parallel threads
        logger: Logger object
        
    Returns:
        Dictionary {ip_address: (session_success, device_count, system_type)}
    """
    if not ip_addresses:
        return {}
        
    unique_ips = list(set([ip for ip in ip_addresses if ip and ip != "-"]))  # Remove duplicates and empty IPs
    results = {}
    
    if not unique_ips:
        logger.warning("No valid IP addresses to check for Dell OME")
        return results
    
    logger.info(f"Starting parallel Dell OME check for {len(unique_ips)} unique IPs using {max_workers} threads")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Start all Dell OME check tasks
        future_to_ip = {executor.submit(check_dell_ome_connection, ip): ip for ip in unique_ips}
        
        # Process results as they complete
        completed = 0
        for future in concurrent.futures.as_completed(future_to_ip):
            ip = future_to_ip[future]
            session_success, device_count, system_type = future.result()
            results[ip] = (session_success, device_count, system_type)
            
            completed += 1
            if completed % 5 == 0 or completed == len(unique_ips):
                logger.info(f"Dell OME check progress: {completed}/{len(unique_ips)} ({round(completed/len(unique_ips)*100)}%)")
    
    # Results summary
    success_count = sum(1 for result in results.values() if result[0])
    
    logger.info(f"Dell OME check completed. Successfully connected to {success_count}/{len(unique_ips)} systems")
    
    return results# Function to check systems in parallel
def check_systems_parallel(ip_addresses: List[str], max_workers: int, logger: logging.Logger) -> Dict[str, Tuple[str, int]]:
    """
    Checks system types for multiple IP addresses in parallel.
    
    Args:
        ip_addresses: List of IP addresses to check
        max_workers: Maximum number of parallel threads
        logger: Logger object
        
    Returns:
        Dictionary {ip_address: (system_type, device_count)}
    """
    if not ip_addresses:
        return {}
        
    unique_ips = list(set([ip for ip in ip_addresses if ip and ip != "-"]))  # Remove duplicates and empty IPs
    results = {}
    
    if not unique_ips:
        logger.warning("No valid IP addresses to check")
        return results
    
    logger.info(f"Starting parallel system check for {len(unique_ips)} unique IPs using {max_workers} threads")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Start all system check tasks
        future_to_ip = {executor.submit(check_system_type, ip, logger): ip for ip in unique_ips}
        
        # Process results as they complete
        completed = 0
        for future in concurrent.futures.as_completed(future_to_ip):
            ip = future_to_ip[future]
            system_type, device_count = future.result()
            results[ip] = (system_type, device_count)
            
            completed += 1
            if completed % 5 == 0 or completed == len(unique_ips):
                logger.info(f"System check progress: {completed}/{len(unique_ips)} ({round(completed/len(unique_ips)*100)}%)")
    
    # Results summary
    hp_count = sum(1 for system_type, _ in results.values() if system_type == "HP OV")
    dell_count = sum(1 for system_type, _ in results.values() if system_type == "Dell OME")
    unknown_count = sum(1 for system_type, _ in results.values() if not system_type)
    
    logger.info(f"System check completed. Found: HP OV: {hp_count}, Dell OME: {dell_count}, Unknown: {unknown_count}")
    
    return results# Function to check system type and retrieve server count for a given IP
def check_system_type(ip_address: str, logger: logging.Logger) -> Tuple[str, int]:
    """
    Attempts to identify the system type (HP OV or Dell OME) and get server count.
    
    Args:
        ip_address: Server IP address
        logger: Logger object
    
    Returns:
        Tuple (system_type, server_count)
    """
    if not ip_address or ip_address == "-":
        return ("", None)
    
    try:
        logger.debug(f"Checking system type for IP: {ip_address}")
        
        # First try HP OneView
        ov_session_success, ov_count, ov_type = check_oneview_connection(ip_address)
        
        if ov_session_success:
            logger.info(f"Successfully identified {ip_address} as HP OneView")
            return (ov_type, ov_count)
            
        # If HP OneView failed, try Dell OME
        dell_session_success, dell_count, dell_type = check_dell_ome_connection(ip_address)
        
        if dell_session_success:
            logger.info(f"Successfully identified {ip_address} as Dell OME")
            return (dell_type, dell_count)
            
        # If both failed, return empty values
        logger.warning(f"Failed to identify system type for IP: {ip_address}")
        return ("", None)
        
    except Exception as e:
        logger.error(f"Error checking system type for {ip_address}: {str(e)}")
        return ("", None)#!/usr/bin/env python3.12
"""
Script for executing GraphQL queries and saving results to CSV.
Query concerns servers from applicationXGenericHardwar with filter on applicationId (175442).
"""

import csv
import json
import logging
import os
import sys
import time
import socket
import subprocess
import concurrent.futures
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple

import requests
from requests.exceptions import RequestException

# Logger configuration
def setup_logger() -> logging.Logger:
    """Configure the logging system."""
    logger = logging.getLogger("graphql_export")
    logger.setLevel(logging.INFO)
    
    # Handler for console logs
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(console_format)
    
    # Handler for file logs
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    file_handler = logging.FileHandler(f"{log_dir}/graphql_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    file_handler.setLevel(logging.DEBUG)
    file_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(file_format)
    
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    return logger

# Function to safely retrieve values from nested dictionaries
def safe_get(data, *keys, default=""):
    """
    Safely retrieves values from nested dictionaries.
    Returns a default value if any element in the path is None.
    
    Args:
        data: Dictionary data
        *keys: Keys for nested access
        default: Default value returned in case of error
        
    Returns:
        Value from the nested dictionary or default value
    """
    current = data
    for key in keys:
        if current is None or not isinstance(current, dict):
            return default
        current = current.get(key)
    return current if current is not None else default

# Function to remove duplicate servers
def remove_duplicate_servers(server_data: List[Dict[str, Any]], key_field: str, logger: logging.Logger) -> List[Dict[str, Any]]:
    """
    Removes duplicate servers based on the specified key field.
    
    Args:
        server_data: List of server data
        key_field: Field name to check for duplicates (e.g., "ServerName")
        logger: Logger object
        
    Returns:
        List of unique servers
    """
    seen_keys = set()
    unique_servers = []
    
    logger.info(f"Before deduplication: {len(server_data)} records")
    
    for server in server_data:
        key_value = server.get(key_field, "")
        if key_value and key_value not in seen_keys:
            seen_keys.add(key_value)
            unique_servers.append(server)
    
    logger.info(f"After deduplication: {len(unique_servers)} unique records")
    return unique_servers

# Function for pinging a server and returning its IP address (thread version)
def ping_server(server_name: str) -> Tuple[str, str]:
    """
    Pings the given server and returns its name and IP address.
    Performs 2 ping attempts for increased reliability.
    
    Args:
        server_name: Server name to ping
        
    Returns:
        Tuple (server_name, ip_address), where ip_address can be an empty string in case of failure
    """
    if not server_name:
        return (server_name, "-")
        
    try:
        # Try to resolve server name to IP address
        try:
            ip_address = socket.gethostbyname(server_name)
            return (server_name, ip_address)
        except socket.gaierror:
            # If name resolution fails, try to ping
            for attempt in range(1, 3):  # 2 ping attempts
                # Determine ping command based on operating system
                if sys.platform.startswith('win'):
                    ping_cmd = ["ping", "-n", "1", "-w", "2000", server_name]
                else:
                    ping_cmd = ["ping", "-c", "1", "-W", "2", server_name]
                    
                # Execute ping command
                process = subprocess.Popen(ping_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                output, error = process.communicate()
                output_str = output.decode('utf-8', errors='ignore')
                
                # Parse ping result to extract IP address
                if process.returncode == 0:
                    # Try to extract IP address from ping result
                    import re
                    ip_pattern = r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'
                    ip_matches = re.findall(ip_pattern, output_str)
                    if ip_matches:
                        ip_address = ip_matches[0]
                        return (server_name, ip_address)
                elif attempt < 2:  # If this is not the last attempt, wait before the next one
                    time.sleep(1)  # Wait 1 second before the next attempt
            
            return (server_name, "-")
    except Exception:
        return (server_name, "-")

# Function for multithreaded server pinging
def get_server_ips_parallel(server_names: List[str], max_workers: int, logger: logging.Logger) -> Dict[str, str]:
    """
    Pings multiple servers in parallel and returns a mapping of server names to IP addresses.
    
    Args:
        server_names: List of server names to ping
        max_workers: Maximum number of parallel threads
        logger: Logger object
        
    Returns:
        Dictionary {server_name: ip_address}
    """
    if not server_names:
        return {}
        
    unique_names = list(set(server_names))  # Remove duplicates
    results = {}
    
    logger.info(f"Starting parallel pinging of {len(unique_names)} unique servers using {max_workers} threads")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Start all pinging tasks
        future_to_server = {executor.submit(ping_server, name): name for name in unique_names}
        
        # Process results as they complete
        completed = 0
        for future in concurrent.futures.as_completed(future_to_server):
            server_name, ip = future.result()
            results[server_name] = ip
            
            completed += 1
            if completed % 10 == 0 or completed == len(unique_names):
                logger.info(f"Pinging progress: {completed}/{len(unique_names)} ({round(completed/len(unique_names)*100)}%)")
    
    # Results summary
    success_count = sum(1 for ip in results.values() if ip and ip != "-")
    logger.info(f"Pinging completed. Found IP addresses for {success_count}/{len(unique_names)} servers")
    
    return results

# Maintained old implementation for compatibility and single calls
def get_server_ip(server_name: str, logger: logging.Logger) -> str:
    """
    Pings the given server and returns its IP address.
    Performs 2 ping attempts for increased reliability.
    
    Args:
        server_name: Server name to ping
        logger: Logger object
        
    Returns:
        Server IP address or "-" in case of error
    """
    if not server_name:
        return "-"
        
    try:
        logger.info(f"Pinging server: {server_name}")
        server_name, ip = ping_server(server_name)
        if ip and ip != "-":
            logger.info(f"Found IP address for {server_name}: {ip}")
        else:
            logger.warning(f"Failed to get IP address for server {server_name}")
        return ip
    except Exception as e:
        logger.error(f"Error while pinging server {server_name}: {str(e)}")
        return "-"

# Function to check Dell OME connection and retrieve device count
def check_dell_ome_connection(ip_address: str) -> Tuple[bool, int, str]:
    """
    Attempts to connect to Dell OME instance and retrieve device count.
    
    Args:
        ip_address: Dell OME server IP address
    
    Returns:
        Tuple (session_success, device_count, system_type)
    """
    if not ip_address or ip_address == "-":
        return (False, None, "")
        
    try:
        # URLs for Dell OME API
        base_url = f"https://{ip_address}/api"
        session_endpoint = f"{base_url}/SessionService/Sessions"
        devices_endpoint = f"{base_url}/DeviceService/Devices"
        
        # Authentication data - replace with proper credentials
        auth_data = {
            "UserName": "admin",
            "Password": "password",
            "SessionType": "API"
        }
        
        # Headers for the request
        headers = {
            "Content-Type": "application/json"
        }
        
        # Disable SSL warnings in development (remove in production)
        requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)
        
        # Execute authentication request
        auth_response = requests.post(session_endpoint, headers=headers, json=auth_data, verify=False, timeout=30)
        
        if auth_response.status_code != 201 and auth_response.status_code != 200:  # Dell OME may return 201 for created session
            return (False, None, "")
            
        # Get session token from header
        session_token = auth_response.headers.get('X-Auth-Token')
        
        if not session_token:
            return (False, None, "")
            
        # Update headers with session token
        headers["X-Auth-Token"] = session_token
        
        # Execute request to devices endpoint
        devices_response = requests.get(devices_endpoint, headers=headers, verify=False, timeout=30)
        
        if devices_response.status_code != 200:
            return (True, None, "Dell OME")  # Got session but failed to get device count
            
        # Get total number of devices from odata.count
        devices_data = devices_response.json()
        device_count = devices_data.get("@odata.count")
        
        return (True, device_count, "Dell OME")
    except Exception:
        return (False, None, "")

# Function to check OneView for multiple IPs in parallel
def check_oneview_connections_parallel(ip_addresses: List[str], max_workers: int, logger: logging.Logger) -> Dict[str, Tuple[bool, int, str]]:
    """
    Checks OneView connections for multiple IP addresses in parallel.
    
    Args:
        ip_addresses: List of IP addresses to check
        max_workers: Maximum number of parallel threads
        logger: Logger object
        
    Returns:
        Dictionary {ip_address: (session_success, server_count, system_type)}
    """
    if not ip_addresses:
        return {}
        
    unique_ips = list(set([ip for ip in ip_addresses if ip and ip != "-"]))  # Remove duplicates and empty IPs
    results = {}
    
    if not unique_ips:
        logger.warning("No valid IP addresses to check for OneView")
        return results
    
    logger.info(f"Starting parallel OneView check for {len(unique_ips)} unique IPs using {max_workers} threads")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Start all OneView check tasks
        future_to_ip = {executor.submit(check_oneview_connection, ip): ip for ip in unique_ips}
        
        # Process results as they complete
        completed = 0
        for future in concurrent.futures.as_completed(future_to_ip):
            ip = future_to_ip[future]
            session_success, server_count, system_type = future.result()
            results[ip] = (session_success, server_count, system_type)
            
            completed += 1
            if completed % 5 == 0 or completed == len(unique_ips):
                logger.info(f"OneView check progress: {completed}/{len(unique_ips)} ({round(completed/len(unique_ips)*100)}%)")
    
    # Results summary
    success_count = sum(1 for result in results.values() if result[0])
    hp_count = sum(1 for result in results.values() if result[2] == "HP OV")
    dell_count = sum(1 for result in results.values() if result[2] == "Dell OME")
    
    logger.info(f"OneView check completed. Successfully connected to {success_count}/{len(unique_ips)} systems")
    logger.info(f"System types: HP OV: {hp_count}, Dell OME: {dell_count}")
    
    return results

# Function to execute GraphQL query
def execute_graphql_query(url: str, query: str, variables: Dict[str, Any], logger: logging.Logger) -> Optional[Dict[str, Any]]:
    """
    Executes a GraphQL query and returns the result.
    
    Args:
        url: GraphQL endpoint URL
        query: GraphQL query text
        variables: Variables for the query
        logger: Logger object
        
    Returns:
        Dictionary with response or None in case of error
    """
    headers = {
        "Content-Type": "application/json",
        # Add additional headers here if required (e.g., authorization)
        # "Authorization": "Bearer YOUR_TOKEN"
    }
    
    payload = {
        "query": query,
        "variables": variables
    }
    
    try:
        logger.info(f"Executing GraphQL query with variables: {variables}")
        response = requests.post(url, json=payload, headers=headers)
        response.raise_for_status()
        
        data = response.json()
        
        if "errors" in data:
            logger.error(f"GraphQL error: {data['errors']}")
            return None
            
        return data
        
    except RequestException as e:
        logger.error(f"Error while executing query: {str(e)}")
        return None
    except ValueError as e:
        logger.error(f"Error while parsing JSON response: {str(e)}")
        return None

# Function to process data and save to CSV
def process_and_save_to_csv(data: Dict[str, Any], logger: logging.Logger) -> bool:
    """
    Processes data from GraphQL response and saves to CSV file.
    
    Args:
        data: Dictionary with response data
        logger: Logger object
        
    Returns:
        True if operation succeeded, False otherwise
    """
    try:
        # Check if data contains expected structure
        if not data or "data" not in data:
            logger.error("No data to process")
            return False
            
        # Adjust this path to your GraphQL data structure
        server_data = data.get("data", {}).get("applicationXGenericHardwar", [])
        
        if not server_data:
            logger
